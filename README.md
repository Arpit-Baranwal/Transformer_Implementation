# Transformer Implementation Hub

A collection of Google Colab notebooks that implement modern Transformer architectures from first principles.

This repository focuses on **understanding transformer internals**, **clean machine-learning engineering**, and **practical application** across NLP and vision tasks. Each notebook is designed to be executable, readable and technically rigorous.

---

## Purpose

The goal of this repository is to bridge the gap between transformer research and real-world implementation by providing:

- End-to-end, executable implementations
- Clear, step-by-step construction of core transformer components
- Minimal abstractions to expose internal mechanics

All notebooks run **directly in Google Colab with zero setup**.

---

## What This Repository Covers

- Transformer fundamentals implemented from scratch  
  (embeddings, attention, masking, residuals, normalization)
- Multi-Head Attention and optimized variants
- Encoder-only, Decoder-only, and Encoderâ€“Decoder architectures
- Training loops, loss functions, and evaluation pipelines
- Applications across:
  - Natural Language Processing
  - Vision and multimodal tasks (where applicable)

---

 ## Notebook-first reproducibility  
  Every implementation runs end-to-end in a single Colab session.

**How to Use**

1. Open any notebook in Google Colab  
2. Run all cells top-to-bottom  
3. Modify components to experiment with architecture or training behavior

No local environment or dependency setup is required.

---

## Repository Status

This repository is actively maintained and extended with:
- New transformer variants
- Optimization techniques
- Experimental notebooks tied to real-world use cases

---
